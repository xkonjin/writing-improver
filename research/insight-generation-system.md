# The Insight Generation System v2

## What Actually Works vs. What Sounds Good

---

## What Failed (v1)

The original system was framework-first: apply Aggregation Theory, 7 Powers, Christensen, then stress-test. This produced three levels of output, each rejected:

**V1 output: "Stablecoins are a deposits business disguised as payments."** Feedback: "pretty good but can do better." Problem: this is a REFRAMING. It follows the template "[Industry X] is actually [Industry Y] in disguise." Swap "stablecoins" for "cloud computing" and "deposits" for "real estate" and the structure still works. Low Kolmogorov complexity.

**V2 output: "Two products sharing one technology — convenience (Circle) vs. access (Tether), with Eurodollar structural parallel."** Feedback: "go deeper, increase the Kolmogorov complexity." Problem: this is FRAMEWORK APPLICATION. Take Christensen's disruption theory + a historical analogy and apply to the domain. Someone who knows the frameworks but nothing about stablecoins could reach the same conclusion. Still derivable.

**V3 output: The Tether dollar hegemony thesis.** Feedback: "very good." What changed: the insight required specific data points (the $15B/year interest savings paper, the Bo Hines 10-day timeline, the Cantor/Lutnick Commerce Secretary nexus, the $17B growing shadow loan book, the 534.5M user count, the GENIUS Act T-bill requirement) that can't be derived from any framework. Remove any single data point and the insight weakens. That's high Kolmogorov complexity.

**The lesson:** Frameworks produce medium-complexity insights. Data produces high-complexity insights. The right process is data-first, frameworks-second.

---

## The Process That Actually Works

### Step 1: SATURATE (80% of the work)

Gather an overwhelming volume of specific data. Four parallel research tracks minimum, each attacking a different facet of the domain. The research must return:

- **Numbers**: dollar amounts, percentages, ratios, growth rates, market shares
- **Names**: specific companies, specific people, specific banks, specific regulators
- **Dates**: when things happened, timelines, sequences of events
- **Mechanisms**: how money actually flows, who pays whom, what settlement systems are involved
- **Anomalies**: things that are growing when they should be shrinking, connections between entities that seem unrelated, pledges that were broken, gaps between public narrative and actual data

What to research for any domain (adapt these tracks):

| Track                          | Focus                                                                                                                     | Example from stablecoin research                                                                                                                                                     |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1. Money/value flow mechanics  | How does value actually move through the system? Who touches it? What's the plumbing?                                     | USDC minting: wire → Cross River Bank → BlackRock Circle Reserve Fund → on-chain mint. Fedwire 3PM ET cutoff. $100K minimum USDT redemption.                                         |
| 2. Power/governance structure  | Who controls what? Who owns whom? What are the relationships between regulators and regulated?                            | Cantor custodies 99% of Tether's $122B in T-bills. Cantor owns 5% of Tether. Cantor's former CEO is Commerce Secretary.                                                              |
| 3. Regulatory architecture     | What does the law actually say? What are the specific requirements, thresholds, exemptions? Who wrote them? Who benefits? | GENIUS Act Section 4: reserves must be T-bills with maturity ≤93 days. Section 4(a)(11): issuers cannot pay yield. Bo Hines helped write it, then became Tether's CEO 10 days later. |
| 4. Comparative/historical data | What happened when something structurally similar existed before? What specific numbers tell the story?                   | Eurodollars: $70B (late 1960s) → $2.2T (1984). Regulation Q capped rates, dollars went offshore. GENIUS Act raises onshore costs, same dynamic.                                      |

**What "saturate" means concretely:** Each research track should return 3,000+ words of specific facts. Not analysis. Not narrative. Facts with sources. If a track returns mostly narrative ("stablecoins are growing rapidly..."), it failed. Send it back for specifics.

**Primary sources over summaries:** SEC filings over news articles. S-1s over analyst takes. Regulatory text over commentary. Academic papers over blog posts. Attestation reports over Twitter threads. The insight is in the primary source data, not in other people's interpretations of it.

### Step 2: IDENTIFY ANOMALIES

Read all the research. Flag anything that:

1. **Contradicts the standard narrative.** Tether pledged to eliminate secured loans by end 2023. They're now $17B and growing. Why?
2. **Connects entities that shouldn't be connected.** Commerce Secretary's family firm custodies $122B for an unregulated offshore company. Why does nobody talk about this?
3. **Shows a number moving in the wrong direction.** Correspondent banking relationships declined 20% since 2011, but dollar usage in emerging markets is growing. Something is replacing the banks.
4. **Reveals a gap between what's said and what's done.** S&P rates USDT stability "5 (weak)" but 534.5M people use it and 35.2M new users joined last quarter.
5. **Shows concentrated flow through a narrow channel.** Cumberland received $23.7B in USDT minting and sent 79% directly to Binance. Two firms received the majority of all USDT ever minted.

The anomalies are the raw material. Write them down as simple factual statements. Don't interpret yet.

### Step 3: CROSS-REFERENCE BETWEEN TRACKS

The insight comes from the INTERSECTION of data points from different research tracks. Look for connections between:

- Track 1 data (mechanics) + Track 2 data (power structure) = who benefits from the mechanics being structured this way?
- Track 3 data (regulation) + Track 4 data (history) = is the regulation recreating a historical pattern?
- Track 1 data (mechanics) + Track 3 data (regulation) = does the regulation create a closed loop in the mechanics?

**The stablecoin example:**

- Track 1: Tether buys $141B in T-bills → Academic paper says this saves the US $15B/year in interest
- Track 3: GENIUS Act requires stablecoins to hold T-bills → This CODIFIES the T-bill demand pipeline into law
- Track 2: Bo Hines wrote the GENIUS Act, then became Tether's CEO → The person who wrote the law now runs the biggest beneficiary
- Track 4: Regulation Q raised onshore banking costs, dollars went offshore, and the US eventually decided the Eurodollar market was useful → GENIUS Act raises onshore stablecoin costs, Tether stays offshore, and the US benefits from the dollarization

Each individual data point is a fact. The CONNECTIONS between them are the insight. The connections require all four tracks to see.

### Step 4: ARTICULATE THE MECHANISM

Don't describe the pattern. Explain the MECHANISM that produces it. A mechanism is a causal chain where each step leads to the next:

**Bad (description):** "Tether has become important to the US Treasury market."

**Bad (framework application):** "Tether has aggregator dynamics because it owns the user relationship in emerging markets."

**Good (mechanism):** "Dollar deposits flow in from 534.5M users in the developing world → Tether buys T-bills ($141B, 18th largest holder globally) → compressed yields save the US ~$15B/year → the US has structural incentive to keep stablecoins flowing → GENIUS Act codifies T-bill backing requirement → more stablecoin issuance = more T-bill demand → depositors get dollar access, Tether keeps ~$10B/year in yield, US gets cheap borrowing."

The mechanism test: Can you trace the dollar from a specific person's wallet through every intermediary to its final resting place, naming each entity and what they gain at each step? If you can, you understand the mechanism. If you can't, you're still describing.

### Step 5: MAKE A FALSIFIABLE PREDICTION

The prediction must be:

1. **Specific** — name the entity, the action, and the timeframe
2. **Falsifiable** — it can be checked and you can be wrong
3. **Non-obvious** — it doesn't follow from the consensus view
4. **Derived from the mechanism** — the prediction is what the mechanism produces if it continues operating

**Bad predictions:**

- "Stablecoins will grow" (consensus, unfalsifiable timeline)
- "Regulation will increase" (obvious, unfalsifiable)
- "Some issuers won't survive" (template, swap any industry)

**Good predictions:**

- "No major US enforcement action will target Tether's offshore USDT operations through end of 2027, because the dollar hegemony benefit (~$15B/year in interest savings + emerging market dollarization) outweighs the regulatory risk."
- "Tether's secured loan book will exceed $30B by end of 2026, because the trade finance function is the actual growth driver."
- "Circle's net margin will compress below 5% by end of 2026, because the Coinbase revenue share ($908M on $1.68B revenue) is a structural constraint that GENIUS Act doesn't fix."

### Step 6: VALIDATE

Run the finished insight through these checks:

**Kolmogorov test:** Can you remove any single data point and still arrive at the same insight? If yes, that data point is decoration. If every fact is load-bearing, the complexity is genuinely high. The stablecoin insight requires: the $15B interest savings paper AND the Bo Hines timeline AND the Cantor/Lutnick nexus AND the GENIUS Act T-bill requirement AND the correspondent banking decline AND the 534.5M user base AND the conglomerate investment list. Remove any one and the picture is incomplete.

**Template test:** Write the insight as a sentence, then try to swap the domain. "The developing world is financing [GOVERNMENT] debt through [COMPANY] at zero yield, [COMPANY] keeps $[X]B/year in profit, and [GOVERNMENT] saves $[X]B/year in interest." Can you fill in the blanks with a different industry? If you can, it's still a template (though it's getting close). If the specific numbers, entities, and mechanisms resist substitution, it passes.

**Pre-mortem test:** It's 2028 and you were wrong. Why? Write three scenarios. Each scenario reveals an assumption you're making. Test each assumption against the data.

**"Wait, that means..." test:** Read the insight to someone who knows the domain. If their reaction is "huh, interesting" — it's description. If their reaction is "wait, so that means [something they hadn't considered]" — it's insight.

---

## Frameworks: When to Use Them (Phase 2, Not Phase 1)

The frameworks from v1 are still useful, but ONLY for validation and stress-testing AFTER the data has produced a candidate insight. They should never be the starting point.

| Framework                             | Use it to...                                                             | Reference                     |
| ------------------------------------- | ------------------------------------------------------------------------ | ----------------------------- |
| Aggregation Theory (Thompson)         | Check: does the insight correctly identify who owns the user?            | `value-capture-frameworks.md` |
| 7 Powers (Helmer)                     | Check: does each claimed moat have both benefit AND barrier?             | `value-capture-frameworks.md` |
| Conservation of Profits (Christensen) | Check: is value migrating between layers as expected?                    | `value-capture-frameworks.md` |
| Commoditize Your Complement (Spolsky) | Check: is someone funding competition in an adjacent layer?              | `value-capture-frameworks.md` |
| Inversion (Munger)                    | Stress-test: how would this company/system fail?                         | Below                         |
| Second-Level Thinking (Marks)         | Stress-test: what does the consensus view miss?                          | Below                         |
| Variant Perception (Steinhardt)       | Structure the insight: idea + consensus + variant + trigger              | Below                         |
| ACH Matrix (CIA)                      | Stress-test: which hypothesis has the least inconsistent evidence?       | Below                         |
| Structure Mapping (Gentner)           | Stress-test analogies: deep structural similarity or surface similarity? | Below                         |
| Pre-Mortem (Klein)                    | Stress-test: imagine failure, reverse-engineer what caused it            | Below                         |

---

## AI Writing Tells to Catch

These crept into the V3 output and must be eliminated in the final written piece.

### Negative Parallelism ("not X, but Y")

The contrastive parallelism with identity reframing pattern. AI deploys this constantly. Max one per piece, and only if the contrast is genuinely surprising.

**Caught in V3 output:**

- "not as payments infrastructure, but as an alternative dollar distribution system"
- "not because regulators won, but because the offshore market proved so useful"
- "not through official bottlers (banks) but through an informal distribution network"
- "The historical parallel isn't a bank or a hedge fund. It's the East India Company"

**Fix:** State what it IS. Don't set up what it ISN'T first. "Tether has become an alternative dollar distribution system" is cleaner than "Tether functions not as payments infrastructure, but as an alternative dollar distribution system."

### Aphorisms and Neat Packaging

The impulse to wrap the insight in a single punchy sentence. This makes it FEEL like a template even when the content has high complexity.

**Caught in V3 output:**

- "The developing world is financing US government debt through Tether at zero yield, Tether keeps $10B+/year in profit, and the US government saves $15B/year in interest — and no one involved has any incentive to change this arrangement."

**Fix:** Let the data speak. Present the mechanism and let the reader arrive at the conclusion. Don't package it for them. The sentence above is the mechanism compressed into an aphorism — better to leave it as the multi-step causal chain and trust the reader to feel the weight.

### The Ascending Reveal Structure

Building toward a dramatic conclusion with headers like "The structural trap (the actual prediction)" or "Why Tether is building a conglomerate (the part that requires first-principles reasoning)." This is theatrical staging. Human experts just state the thing.

**Fix:** Flat structure. Present findings in order of importance, not in order of dramatic tension. The parenthetical sub-headers that telegraph "this is the smart part" are a tell.

### Em-Dash Overuse

ChatGPT/Claude default. Count the em-dashes. If there are more than 3-4 per 1000 words, replace most with commas, parentheses, or sentence breaks.

### Uniform Section Length

Each section of the V3 output was roughly the same length. Human writers spend 80% of the piece on the part that matters most and compress the rest.

### Other Tells to Watch

From `anti-ai-writing-detection.md`:

- Trailing present participles ("...highlighting the importance of...")
- "Furthermore" / "Moreover" / "Additionally" as paragraph openers
- Perfect grammar throughout (break rules deliberately)
- The "Challenges" pattern ("Despite X, Y faces challenges...")
- Motivational closers
- Hedging ("it could potentially be argued")
- Rule of Three where One or Two suffice

---

## Quick Reference: The 6-Step Process

1. **SATURATE** — 4+ parallel research tracks, each returning 3,000+ words of specific facts. Numbers, names, dates, mechanisms. Not narratives.
2. **FIND ANOMALIES** — What contradicts the standard narrative? What connects entities that shouldn't be connected? What's moving in the wrong direction?
3. **CROSS-REFERENCE** — The insight is at the intersection of data from different tracks. Mechanics + power structure. Regulation + history. Flow + incentive.
4. **ARTICULATE THE MECHANISM** — Trace the dollar from wallet to final resting place, naming every entity and what they gain at each step.
5. **PREDICT** — Specific entity, specific action, specific timeframe, derived from the mechanism.
6. **VALIDATE** — Kolmogorov test (every data point is load-bearing), template test (can't swap domains), pre-mortem test (three failure scenarios), "wait that means..." test.

Then: run the written output through the AI writing tell checklist before publishing. Kill negative parallelism, aphorisms, ascending reveals, em-dash overuse, uniform sections, and perfect grammar.

---

## Model Usage: Which Model for Each Step

The V3 stablecoin run used Opus for everything, including 4 research agents that ran 15-18 minutes each doing web searches. That's expensive and unnecessary. The research agents compile facts — they don't need deep reasoning. Reserve the expensive model for the steps where reasoning quality actually matters.

### Model assignments

| Step                          | Model                                                      | Why                                                                                                                                                                                                                           | Cost vs all-Opus                                                |
| ----------------------------- | ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| 1. SATURATE (research agents) | **Sonnet**                                                 | Web search + fact compilation. Follows instructions well, returns structured data. Doesn't need creative reasoning.                                                                                                           | ~70% savings per agent. With 4 agents, this is the biggest win. |
| 2. FIND ANOMALIES             | **Sonnet**                                                 | Pattern matching against specific heuristics (contradictions, wrong-direction numbers, gaps). Structured checklist work.                                                                                                      | ~70% savings                                                    |
| 3. CROSS-REFERENCE            | **Opus**                                                   | This is where the insight emerges. Connecting data across tracks requires the strongest associative reasoning. The difference between V2 ("framework application") and V3 ("genuine insight") happened here. Don't cheap out. | Baseline                                                        |
| 4. ARTICULATE MECHANISM       | **Opus**                                                   | Writing the causal chain with precision. Each link in the chain must be logically tight. Sonnet tends to handwave at the connections.                                                                                         | Baseline                                                        |
| 5. PREDICT                    | **Sonnet**                                                 | Given a clear mechanism from Step 4, deriving predictions is structured work. Prompt with the mechanism and ask for specific/falsifiable/timebound predictions. Opus only if predictions feel generic.                        | ~70% savings                                                    |
| 6. VALIDATE                   | **Haiku** for mechanical checks, **Sonnet** for pre-mortem | Kolmogorov test (remove each data point, check if insight holds) is mechanical. Template test (swap domain, check if it breaks) is mechanical. Pre-mortem requires moderate reasoning.                                        | ~85% savings on mechanical checks                               |
| 7. AI TELL CLEANUP            | **Haiku**                                                  | Scan for banned words, count em-dashes, flag "not X, but Y" patterns, check section length uniformity. Pure checklist.                                                                                                        | ~85% savings                                                    |
| 8. FINAL WRITING              | **Opus**                                                   | Voice, rhythm, asymmetric structure, deliberate imperfection. This is where model quality shows in the output.                                                                                                                | Baseline                                                        |

### Practical implementation in Claude Code

```
# Step 1: Launch research agents with Sonnet (not Opus)
Task(subagent_type="general-purpose", model="sonnet", prompt="Research [track]...")

# Step 2: Anomaly detection with Sonnet
Task(subagent_type="general-purpose", model="sonnet", prompt="Read these research outputs. Flag anomalies: contradictions, unexpected connections, numbers moving wrong direction, gaps between narrative and data. Return as simple factual statements, no interpretation...")

# Step 3-4: Cross-reference and mechanism with Opus (default, or explicit)
# Do this in the main conversation thread, not a subagent.
# The main thread has all research context loaded.
# Opus is the default model for the main thread.

# Step 5: Predictions with Sonnet
Task(subagent_type="general-purpose", model="sonnet", prompt="Given this mechanism: [paste mechanism]. Generate 4-5 falsifiable predictions. Each must name a specific entity, specific action, specific timeframe...")

# Step 6-7: Validation and tell cleanup with Haiku
Task(subagent_type="general-purpose", model="haiku", prompt="Run these validation checks on the following insight: [paste]. For each check, return PASS or FAIL with explanation...")
Task(subagent_type="general-purpose", model="haiku", prompt="Scan this text for AI writing tells. Check for: negative parallelism ('not X, but Y'), aphorisms, em-dash count, section length uniformity, banned words from this list: [paste ban list]...")

# Step 8: Final writing stays in main Opus thread
```

### Cost estimate comparison

For a run like the stablecoin analysis (4 research agents + synthesis + validation):

| Configuration                                                 | Estimated cost | Quality                                                                                                                                       |
| ------------------------------------------------------------- | -------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| All Opus (what V3 did)                                        | ~$15-20        | Overkill on research, good on synthesis                                                                                                       |
| Optimized (Sonnet research, Opus synthesis, Haiku validation) | ~$5-7          | Same insight quality. Research agents return same facts. Synthesis stays Opus.                                                                |
| All Sonnet                                                    | ~$3-5          | Research fine. Synthesis noticeably weaker — likely produces V2-quality output (framework application, not genuine insight). Not recommended. |
| All Haiku                                                     | ~$1-2          | Research misses nuance. Synthesis fails. Don't.                                                                                               |

The sweet spot: **Sonnet for Steps 1-2, Opus for Steps 3-4 and 8, Sonnet for Step 5, Haiku for Steps 6-7.** Roughly 3x cheaper than all-Opus with no quality loss on the insight itself.

---

## Appendix: The Stablecoin Case Study (What Each Step Produced)

### Step 1 output: Four research tracks

| Track                       | Key data points gathered                                                                                                                                                                                                                                             |
| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Tether reserves & strategy  | $192.9B reserves, $17B secured loans (growing, pledge to eliminate broken), 96,184 BTC, 127.5 tons gold, $20B+ investment portfolio (Rumble $775M, Northern Data $1B, Adecoagro $615M, Blackrock Neurotech $200M), 534.5M users, $10B+ profit on ~235 employees      |
| Banking/settlement plumbing | USDC: Circle Mint → BNY Mellon/Cross River → BlackRock Reserve Fund. USDT: $100K min, Deltec/Britannia (Bahamas). Cumberland ($23.7B minting, 79% → Binance). 51% USDT on Tron. $27T locked in nostro/vostro globally. Correspondent banking down 20% since 2011.    |
| Regulatory architecture     | GENIUS Act: reserves must be T-bills ≤93 days, no yield to holders, foreign issuers must register with OCC. Bo Hines: White House Aug 9 → Tether Aug 19 → CEO Sep 12. MiCA: Tether delisted from EU exchanges. SEC April 2025: non-yield stablecoins not securities. |
| Comparative/systemic        | $141B T-bills = 18th largest holder globally. Academic paper: $15B/year US interest savings. Cantor: 99% custody, 5% equity, Commerce Secretary's family firm. IMF: "reinforces US world banker balance sheet." Italy FM: "more dangerous than tariffs."             |

### Step 2 output: Anomalies

1. Secured loans: $8.2B → $17B in one year, despite pledge to eliminate. Borrowers undisclosed.
2. Cantor/Lutnick: 99% custody + 5% equity + Commerce Secretary + son chairs 21 Capital. One family, both sides.
3. Bo Hines: wrote the law, then joined the biggest beneficiary, 10-day gap.
4. S&P rates "5 (weak)" but 35.2M new users per quarter. Adoption metrics and risk metrics diverge.
5. Tether buying gold (27 tons/quarter) and BTC while issuing a dollar-backed stablecoin. Hedging against their own backing asset.
6. GENIUS Act requires T-bill backing = codified demand pipeline for US debt. Law as subsidy mechanism.
7. Correspondent banking shrinking in exactly the markets where USDT is growing. Tether fills the gap the banks leave.

### Step 3 output: Cross-references

- (Plumbing) $141B in T-bills + (Systemic) $15B/year interest savings + (Regulation) GENIUS Act requires T-bill backing = **closed loop: law creates demand for US debt**
- (Power) Cantor/Lutnick/Commerce Secretary + (Regulation) Bo Hines revolving door + (Plumbing) 99% single custodian = **political architecture that makes enforcement structurally implausible**
- (Plumbing) correspondent banking decline + (Reserves) 534.5M users, 51% on Tron + (Systemic) IMF "reinforces US world banker" = **Tether replaces the banking system's dollar distribution function**
- (Reserves) $20B conglomerate (gold, farmland, AI, media) + (Regulation) GENIUS Act raises onshore costs + (History) East India Company pattern = **converting temporary monopoly profit into permanent assets**

### Step 4 output: The mechanism

Deposits from 534.5M people in countries with weak currencies → Tether buys T-bills ($141B, saving US ~$15B/year in interest) → GENIUS Act codifies this demand pipeline → US benefits from dollarization without spending a dollar → Cantor/Lutnick nexus + Bo Hines revolving door make enforcement politically complicated → Tether operates dual-track (USDT offshore, USAT onshore) → Keeps $10B+/year in yield while depositors get zero → Converts profits into permanent assets (gold, farmland, AI, media, BTC) hedging against their own obsolescence.

### Step 5 output: Predictions

1. No major US enforcement against Tether's offshore USDT through end 2027.
2. Tether's secured loans exceed $30B by end 2026.
3. Tether's non-reserve investment portfolio exceeds $40B by end 2026.
4. Trigger event to watch: does a CBDC in Nigeria, Turkey, or Argentina actually reduce USDT usage? If not, the "too useful to regulate" thesis holds.

### Step 6 output: Validation

Kolmogorov: Remove the $15B interest savings paper → can't explain why the US tolerates Tether. Remove Bo Hines → can't explain the regulatory capture mechanism. Remove Cantor/Lutnick → can't explain the custody concentration risk. Remove correspondent banking decline → can't explain why Tether is growing in specific markets. Every data point is load-bearing. Passes.

Template: Try swapping "Tether" for "Airbnb" and "stablecoins" for "housing." The mechanism breaks immediately because the T-bill pipeline, Commerce Secretary nexus, and bearer instrument properties are domain-specific. Passes.

Pre-mortem (2028, the thesis was wrong):

1. "Tether had a bank run and the Fed intervened, proving the systemic risk was real and the US did act." → Tests the assumption that the benefit outweighs the risk.
2. "CBDC adoption in Nigeria/Turkey actually replaced USDT, proving the 'too useful to regulate' thesis wrong." → Tests the assumption that CBDCs can't replicate USDT's informal distribution.
3. "DOJ indicted Tether executives and forced a wind-down, dollar hegemony be damned." → Tests the assumption that political connections prevent enforcement.
